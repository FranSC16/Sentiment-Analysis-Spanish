{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5fdfe5",
   "metadata": {},
   "source": [
    "# Comprobación de la GPU para trabajar con ella en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1e783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices()) # list of DeviceAttributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b94e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef8ab78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68620909, 0.73577583, 0.80494183, 0.62175556, 0.00288401,\n",
       "        0.27226369, 0.90287068, 0.10781395, 0.42622504, 0.10732877],\n",
       "       [0.31386874, 0.73673848, 0.02024072, 0.75468158, 0.28102462,\n",
       "        0.12875009, 0.51524577, 0.20628023, 0.83718386, 0.5662807 ],\n",
       "       [0.87488084, 0.47620559, 0.17166554, 0.90519973, 0.54884445,\n",
       "        0.14395166, 0.096056  , 0.5594298 , 0.6567992 , 0.51465907],\n",
       "       [0.30394439, 0.85340647, 0.96867298, 0.24754585, 0.03634093,\n",
       "        0.71209165, 0.64251238, 0.81120581, 0.71979337, 0.46019108],\n",
       "       [0.29502059, 0.23805267, 0.53726039, 0.25211828, 0.80212986,\n",
       "        0.94097916, 0.05323931, 0.61903927, 0.15006856, 0.55467883],\n",
       "       [0.65283496, 0.57470217, 0.29367334, 0.51699057, 0.52455711,\n",
       "        0.63191835, 0.23179804, 0.8197632 , 0.66799701, 0.35933461],\n",
       "       [0.1598239 , 0.60227914, 0.62668446, 0.46007629, 0.93786589,\n",
       "        0.28304774, 0.1804242 , 0.47308841, 0.28778953, 0.87588674],\n",
       "       [0.27264171, 0.62212388, 0.93252097, 0.40550508, 0.27492673,\n",
       "        0.8506555 , 0.41446826, 0.53378946, 0.54318543, 0.17300918],\n",
       "       [0.74255167, 0.01903866, 0.70799431, 0.5872083 , 0.06721688,\n",
       "        0.89589847, 0.59857339, 0.99842584, 0.68998525, 0.59778241],\n",
       "       [0.19701308, 0.26170132, 0.1309687 , 0.31795654, 0.29232849,\n",
       "        0.95110357, 0.13322102, 0.53796528, 0.52457243, 0.66653007],\n",
       "       [0.52880024, 0.61580012, 0.78238168, 0.32295533, 0.48234147,\n",
       "        0.3287536 , 0.0194966 , 0.63956026, 0.92752644, 0.94910078],\n",
       "       [0.36282547, 0.07241519, 0.20252541, 0.18705989, 0.30518025,\n",
       "        0.47269508, 0.91699533, 0.72007586, 0.1562449 , 0.62041979],\n",
       "       [0.3724799 , 0.31697377, 0.25317131, 0.37884347, 0.81486468,\n",
       "        0.93324146, 0.41098127, 0.39418084, 0.83047486, 0.70062169],\n",
       "       [0.77530009, 0.57598151, 0.60973634, 0.44057037, 0.46552737,\n",
       "        0.34023387, 0.45575232, 0.4373379 , 0.59266848, 0.22363801],\n",
       "       [0.26003665, 0.77049984, 0.25456074, 0.2986854 , 0.32926075,\n",
       "        0.26438677, 0.73052171, 0.89686223, 0.67749122, 0.50419456],\n",
       "       [0.16307435, 0.95042189, 0.31695483, 0.61765112, 0.26088381,\n",
       "        0.00385522, 0.13069506, 0.9756076 , 0.72123406, 0.21856886],\n",
       "       [0.34122321, 0.59370516, 0.66909519, 0.83391293, 0.15360897,\n",
       "        0.8675882 , 0.28379296, 0.8418244 , 0.83414949, 0.58137927],\n",
       "       [0.78085488, 0.78819135, 0.3170697 , 0.59927174, 0.63801465,\n",
       "        0.88625743, 0.39244791, 0.69691955, 0.29636433, 0.42538668],\n",
       "       [0.81163066, 0.45973158, 0.90071903, 0.75734443, 0.82321854,\n",
       "        0.9807369 , 0.1248722 , 0.7394163 , 0.44428732, 0.22027353],\n",
       "       [0.1720664 , 0.68583009, 0.77449716, 0.74250942, 0.90384264,\n",
       "        0.61699619, 0.6409879 , 0.29577128, 0.10689575, 0.40342549],\n",
       "       [0.81484641, 0.95730145, 0.62331202, 0.29168308, 0.40650416,\n",
       "        0.09313376, 0.86514271, 0.18867826, 0.41856406, 0.54360568],\n",
       "       [0.84797508, 0.70976174, 0.9413471 , 0.80871577, 0.60693562,\n",
       "        0.28926634, 0.38140732, 0.68992578, 0.95272059, 0.81710556],\n",
       "       [0.21206443, 0.21265221, 0.32383968, 0.25646176, 0.13550743,\n",
       "        0.04302005, 0.18975195, 0.43507074, 0.97603136, 0.13722867],\n",
       "       [0.58902427, 0.61649393, 0.41832468, 0.71030248, 0.5138443 ,\n",
       "        0.23457977, 0.50062812, 0.754109  , 0.81798907, 0.4568506 ],\n",
       "       [0.54980255, 0.28366448, 0.70213883, 0.91860405, 0.48802321,\n",
       "        0.99580825, 0.97492246, 0.05020494, 0.2547354 , 0.17749091],\n",
       "       [0.93154654, 0.49009051, 0.91261308, 0.23692039, 0.80301766,\n",
       "        0.51136761, 0.50468766, 0.63754813, 0.83888966, 0.61503057],\n",
       "       [0.40335737, 0.22223781, 0.12871022, 0.60897479, 0.70663784,\n",
       "        0.65435321, 0.8718937 , 0.45868709, 0.86742073, 0.80364613],\n",
       "       [0.8281275 , 0.40309654, 0.65940322, 0.79791815, 0.93468771,\n",
       "        0.85384343, 0.76943779, 0.23708153, 0.82050279, 0.55948796],\n",
       "       [0.03523094, 0.22782147, 0.29672084, 0.67701676, 0.34705487,\n",
       "        0.28343354, 0.50544201, 0.57154038, 0.03369988, 0.22997812],\n",
       "       [0.47028682, 0.64771103, 0.76923429, 0.40501021, 0.86129446,\n",
       "        0.91668636, 0.46854515, 0.22855042, 0.48924944, 0.69134989],\n",
       "       [0.76265293, 0.79826225, 0.13210568, 0.89838979, 0.06818788,\n",
       "        0.89788822, 0.76158827, 0.77219912, 0.00426373, 0.28170526],\n",
       "       [0.76844357, 0.39571071, 0.92506951, 0.81415656, 0.30033834,\n",
       "        0.99673724, 0.94167067, 0.55383642, 0.60078366, 0.69971998]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_array = np.random.rand(32, 10)\n",
    "input_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dd5f1",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1036185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from datasets import load_dataset\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk as nl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9f764",
   "metadata": {},
   "source": [
    "# Dataset corregido a mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474af407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datasets, y leo el fichero excel\n",
    "from sklearn.utils import shuffle\n",
    "archivo = 'BBDD_End.xlsx'\n",
    "df = pd.read_excel(archivo, sheet_name='Coms_train')\n",
    "df2 = pd.read_excel(archivo, sheet_name='Coms_val')\n",
    "df3 = pd.read_excel(archivo, sheet_name='Coms_test')\n",
    "print(df)\n",
    "print(df2)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a visualizar las frecuencias absolutas de las clases histograma\n",
    "import matplotlib.pyplot as plot\n",
    "from random import random\n",
    "import numpy as np\n",
    "array = np.array(df['labels'])\n",
    "(uniques, counts) = np.unique(array, return_counts = True)\n",
    "print(f'Unicos: {uniques}')\n",
    "print(f'Counts: {counts}')\n",
    "\n",
    "#Datos aleatorios para el ejemplo\n",
    "labels = list(df['labels'])\n",
    "eje_x = ['Negativo','Positivo']\n",
    "eje_y = [4044,3956]\n",
    "fig = plt.bar(eje_x, eje_y, color='g')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xlabel('Clases')\n",
    "plt.title('Distribución de clases')\n",
    "plt.savefig('Clases.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparto datos entre datos de entrenamiento y de testeo con una proporción 77-23, y las respectivas etiquetas\n",
    "train_data, train_data_val, test_data = df['data'], df2['data'], df3['data']\n",
    "train_labels, train_labels_val, test_labels = df['labels'], df2['labels'], df3['labels']\n",
    "print(test_data,test_labels,train_data,train_labels,train_data_val,train_labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83242aa",
   "metadata": {},
   "source": [
    "# Data Augmentation para NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation para NLP\n",
    "detokenizer = Detok()\n",
    "jw = Jaccard()\n",
    "fdist = FreqDist()\n",
    "nlp = es_core_news_sm.load()\n",
    "url = 'https://www.wordreference.com/sinonimos/'\n",
    "sentence_syn_d = []\n",
    "tag_vector = ['NOUN','ADJ']\n",
    "tag_anterior = ['ADJ','NOUN'] # Polisemia pero para verbos\n",
    "tag_posterior = ['ADJ','NOUN','ADV','ADP','CCONJ','SCONJ'] # Polisemia para verbos\n",
    "words_preps = ['a','e','o','ante','bajo','cabe','con','contra','de','desde',\\\n",
    "'en','entre','hacia','hasta','y','ni','para','por','según','si','so','sobre','mediante','durante']\n",
    "words_out = ['no','si','gracias','más','años','que','se']\n",
    "i = 0\n",
    "for sentence in data_origin:\n",
    "    if (i%2) == 1:\n",
    "        tag_vector = ['NOUN','ADJ']\n",
    "    else:\n",
    "        tag_vector = ['NOUN']\n",
    "    print('Frase: ',+i)\n",
    "    i += 1\n",
    "    print('Frase original: '+str(sentence.lower().replace(\",\",\"\")))\n",
    "    sentence_tok = word_tokenize(sentence.lower())\n",
    "    sentence_syn = []\n",
    "    # Tags de toda la oración\n",
    "    sentence_tag = nlp(sentence)\n",
    "    for token in sentence_tok:\n",
    "        token_tag = nlp(token)\n",
    "        index = sentence_tok.index(token)\n",
    "        if index == len(sentence_tok):\n",
    "            index = index - 1\n",
    "        if token_tag[0].pos_ in str(tag_vector) and token not in str(words_out) and token not in str(words_preps):\n",
    "            buscar = url+token.lower() \n",
    "            try:\n",
    "                resp = requests.get(buscar, timeout = 10)\n",
    "                time.sleep(2)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(e)\n",
    "                print('fracaso')\n",
    "                continue\n",
    "            soup = BeautifulSoup(resp.text)\n",
    "            lista = soup.find_all(class_=\"trans clickable\")\n",
    "            longitud_lista = len(lista)\n",
    "            resp.close()\n",
    "            if longitud_lista == 0:\n",
    "                sentence_syn.append(token)\n",
    "            else:\n",
    "                ok = True\n",
    "                if index+1>len(sentence_tok) or index-1<0:\n",
    "                    ok = False\n",
    "                tag_correcto = []\n",
    "                tag_comprueba = str(token_tag[0].pos_+\" \"+str(0))                            \n",
    "                if longitud_lista >= 2:                    \n",
    "                    print(\"Polisemia de la palabra: \"+str(token))\n",
    "                    tag_correcto = tag_comprueba\n",
    "                else:\n",
    "                    tag_correcto = tag_comprueba\n",
    "                sv = {}\n",
    "                synonyms_vector = []\n",
    "                for h in range(len(lista)):\n",
    "                    soup2 = BeautifulSoup(str(lista[h]))\n",
    "                    lista2 = soup2.find_all('li')\n",
    "                    len_lista = len(lista2)\n",
    "                    a = 0\n",
    "                    for j in range(len_lista):\n",
    "                        tag_actual = word_tokenize(str(lista2[j].next_element).replace(\",\",\"\"))\n",
    "                        longitud_sinonimos = len(tag_actual)\n",
    "                        tag_actual = detokenizer.detokenize(tag_actual)\n",
    "                        tag_actual = nlp(tag_actual)\n",
    "                        if str(tag_actual[0]) != '<': \n",
    "                            for m in range(longitud_sinonimos):\n",
    "                                fdist[tag_actual[m].pos_] += 1\n",
    "                            values = list(fdist.values())\n",
    "                            index = values.index(max(values))\n",
    "                            tag_mc = list(fdist.keys())[index]\n",
    "                            fdist.clear()\n",
    "                            if j > 0 and j <= len_lista-2:\n",
    "                                tag_posterior = nlp(str(lista2[j+1].next_element))\n",
    "                                if tag_posterior[0].pos_ != tag_actual[0].pos_:\n",
    "                                    a = 0                \n",
    "                            sv[tag_mc+\" \"+str(a)] = str(lista2[j].next_element).replace(\",\",\"\")\n",
    "                            a = a + 1                        \n",
    "                if tag_comprueba not in sv.keys():\n",
    "                    lista_tags_dic = list(sv.keys())\n",
    "                    vector_sim = []\n",
    "                    for u in range(len(lista_tags_dic)):\n",
    "                        vector_sim.append(jw.similarity(lista_tags_dic[u],tag_comprueba,1.25))    \n",
    "                    maxim = min(vector_sim)\n",
    "                    index = vector_sim.index(maxim)\n",
    "                    tag_correcto = lista_tags_dic[index]\n",
    "                synonyms_vector.append(word_tokenize(sv[tag_correcto]))\n",
    "                sv = {}\n",
    "                syn_sim = []\n",
    "                if i%2 == 0:\n",
    "                    jw = Jaccard()\n",
    "                else:\n",
    "                    jw = Jaro()                    \n",
    "                for syn in synonyms_vector[0]:\n",
    "                    syn_sim.append(jw.similarity(token,syn,1.25))                \n",
    "                maximo = max(syn_sim)\n",
    "                index = syn_sim.index(maximo)\n",
    "                sentence_syn.append(synonyms_vector[0][index])\n",
    "        else:\n",
    "            sentence_syn.append(token)\n",
    "     \n",
    "    sentence_syn_dtok = detokenizer.detokenize(sentence_syn)\n",
    "    print('Frase clonada: '+str(sentence_syn_dtok))\n",
    "    \n",
    "    sentence_syn_d.append(sentence_syn_dtok)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341db89",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784582bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "words_preps = ['a','e','o','ante','bajo','cabe','con','contra','de','desde',\\\n",
    "'en','entre','hacia','hasta','y','ni','para','por','según','si','so','sobre','mediante','durante']\n",
    "detokenizer = Detok()\n",
    "b = True\n",
    "train_data_corregido = []\n",
    "train_data_corregido_detok = []\n",
    "fdist = FreqDist()\n",
    "for sentence in train_data:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "#     print(tokens)\n",
    "    tokens_aux = []\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            if token in stopwords.words('spanish'):\n",
    "                b = False\n",
    "                break\n",
    "            elif char.isnumeric():\n",
    "                b = False\n",
    "                break\n",
    "            elif char in string.punctuation:\n",
    "                b = False\n",
    "                break\n",
    "            elif token in words_preps:\n",
    "                b = False\n",
    "                break\n",
    "        if b:\n",
    "            tokens_aux.append(spanish_stemmer.stem(token))\n",
    "            #tokens_aux.append(token)\n",
    "            fdist[spanish_stemmer.stem(token)] +=1\n",
    "            #fdist[token] += 1\n",
    "        b = True\n",
    "#     print(tokens_aux)\n",
    "    train_data_corregido.append(tokens_aux)\n",
    "    train_data_corregido_detok.append(detokenizer.detokenize(tokens_aux))\n",
    "    \n",
    "train_data_corregido_detok[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04023f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este bucle me quita las stopwords en español de los datos de entrenamiento\n",
    "# Ademas de quitar algunos signos inútiles, y almacenar los lexemas\n",
    "b = True\n",
    "train_data_corregido_val = []\n",
    "fdist2 = FreqDist()\n",
    "for sentence in train_data_val:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens_aux = []\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            if token in stopwords.words('spanish'):\n",
    "                b = False\n",
    "                break\n",
    "            elif char.isnumeric():\n",
    "                b = False\n",
    "                break\n",
    "            elif char in string.punctuation:\n",
    "                b = False\n",
    "                break\n",
    "        if b:\n",
    "            tokens_aux.append(spanish_stemmer.stem(token))\n",
    "            fdist2[spanish_stemmer.stem(token)] +=1\n",
    "        b = True\n",
    "    train_data_corregido_val.append(tokens_aux)\n",
    "print('Oración original\\n: '+str(train_data_val[0])+\"\\n\")\n",
    "print('Oración resultante\\n: '+str(train_data_corregido_val[0])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5685f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Me deshago de todas las palabras consideradas stopwords, en español en los datos de testeo\n",
    "test_data_corregido = []\n",
    "fdist3 = FreqDist()\n",
    "b = True\n",
    "for sentence in test_data:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens_aux = []\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            if token in stopwords.words('spanish'):\n",
    "                b = False\n",
    "                break\n",
    "            elif char in string.punctuation:\n",
    "                b = False\n",
    "                break\n",
    "            elif char.isnumeric():\n",
    "                b = False\n",
    "                break\n",
    "        if b:\n",
    "            tokens_aux.append(spanish_stemmer.stem(token))\n",
    "            fdist3[spanish_stemmer.stem(token)] += 1 \n",
    "        b = True\n",
    "    test_data_corregido.append(tokens_aux)\n",
    "print('Oración original\\n: '+str(test_data[73])+\"\\n\")\n",
    "print('Oración resultante\\n: '+str(test_data_corregido[0])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677aae96",
   "metadata": {},
   "source": [
    "# Creación de vocabulario (train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1962377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_words[:10]\n",
    "lista_palabras = []\n",
    "lista_conteo_m1 = []\n",
    "lista_conteo_m2 = []\n",
    "train_mcw_m1 = fdist.most_common(100000)\n",
    "train_mcw_m2 = fdist.most_common(1000000)\n",
    "for tup in train_mcw_m1:\n",
    "    lista_conteo_m1.append(tup[1])\n",
    "for tup2 in train_mcw_m2:\n",
    "    lista_palabras.append(tup2[0])\n",
    "    lista_conteo_m2.append(tup2[1])\n",
    "lista_conteo_m1 = np.array(lista_conteo_m1)\n",
    "lista_conteo_m2 = np.array(lista_conteo_m2)\n",
    "umbral_m1 = np.around(np.mean(lista_conteo_m1))\n",
    "umbral_m2 = np.around(np.mean(lista_conteo_m2))\n",
    "umbral = np.around(0.00003*umbral_m1*0.99997*umbral_m2)\n",
    "# print(umbral)\n",
    "vocab = []\n",
    "for token in train_mcw_m2:\n",
    "    if token[1]>umbral:\n",
    "        vocab.append(token[0])\n",
    "print(len(vocab))\n",
    "with open(\"vocab_size.txt\",'a') as f:\n",
    "    f.write(str(len(vocab))+\" \")\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aumentar el vocabulario, a el mayor de palabras posibles\n",
    "vocab_size = len(vocab)+10\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "# Este comando asigna a cada palabra distinta que encuentra en los datos de entrenamiento, un número identificativo,\n",
    "# es decir, crea el vocabulario\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cff5b9",
   "metadata": {},
   "source": [
    "# Normalización de longitud de oración (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto me sustituye los datos de entrada, por su correspondiente número dentro del vocabulario\n",
    "# El padding, es la normalización de las oraciones a una longitud fija, cortándola si superar dicha longitud o añadiendo 0 si\n",
    "# no llega\n",
    "max_length1 = max([len(row) for row in train_data_corregido])\n",
    "max_length2 = max([len(row) for row in train_data_corregido_val])\n",
    "max_length3 = max([len(row) for row in test_data_corregido])\n",
    "max_length = max(max_length1,max_length2,max_length3)\n",
    "print(\"Maximum length data:\",max_length)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences = tokenizer.texts_to_sequences(train_data_corregido)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "padded = np.array(padded)\n",
    "sequences_val = tokenizer.texts_to_sequences(train_data_corregido_val)\n",
    "padded_val = pad_sequences(sequences_val, maxlen = max_length, truncating=trunc_type)\n",
    "padded_val = np.array(padded_val)\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_data_corregido)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
    "testing_padded = np.array(testing_padded)\n",
    "print(padded[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec00ae2",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f274f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)\n",
    "embbeding_size = 20\n",
    "#-# Primera Forma de definir la red neuronal - Modelo V2\n",
    "model = tf.keras.Sequential([\n",
    "    # Digamos que la capa embedding es una matriz de input_length x embedding_size siendo filas y columnas respectivamente.\n",
    "    tf.keras.layers.Embedding(vocab_size, embbeding_size, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64,activation ='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32,activation ='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(16,activation ='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1,activation = 'sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dff136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queda pendiente mejorar la estructura de la red neuronal, incluso aumentando las capas, aplicando max pooling, informándose\n",
    "# sobre la capa de incrustración, y mejorando los gradientes y viendo pseudocódigos de otras redes neuronales.\n",
    "embbeding_size = 15\n",
    "#-# Primera Forma de definir la red neuronal - Modelo V2\n",
    "model = tf.keras.Sequential([\n",
    "    # Digamos que la capa embedding es una matriz de input_length x embedding_size siendo filas y columnas respectivamente.\n",
    "    tf.keras.layers.Embedding(vocab_size, embbeding_size, input_length=max_length),\n",
    "    #tf.keras.layers.LSTM(100, return_sequences = True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128)),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1,activation = 'sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6627f34",
   "metadata": {},
   "source": [
    "# Entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e56c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Entrenamiento y validación\n",
    "num_epochs = 180\n",
    "#es = EarlyStopping(monitor='val_loss', patience=6)\n",
    "opt2 = tf.keras.optimizers.Adam(learning_rate=0.00000368) # 0.000009988\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = 0.05, momentum = 0.96, decay = 0.0000000001);\n",
    "model.compile(loss='mse', optimizer=opt2, metrics=['accuracy'])\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "# sentence_vectors = np.array(sentence_vectors)\n",
    "# sentence_vectors_test = np.array(sentence_vectors_test)\n",
    "history = model.fit(padded, train_labels, epochs = num_epochs, validation_data = (padded_val, train_labels_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45e8c5",
   "metadata": {},
   "source": [
    "# Visualización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualize_Result(acc, val_acc, loss, val_loss):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1,\n",
    "                                   ncols=2,\n",
    "                                   figsize=(15, 6),\n",
    "                                   sharex=True)\n",
    "\n",
    "    plot1 = ax1.plot(range(0, len(acc)),\n",
    "                     acc,\n",
    "                     label='accuracy')\n",
    "\n",
    "    plot2 = ax1.plot(range(0, len(val_acc)),\n",
    "                     val_acc,\n",
    "                     label='val_accuracy')\n",
    "\n",
    "    ax1.set(title='Accuracy And Val Accuracy progress',\n",
    "            xlabel='epoch',\n",
    "            ylabel='accuracy/ validation accuracy')\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    plot3 = ax2.plot(range(0, len(loss)),\n",
    "                     loss,\n",
    "                     label='loss')\n",
    "\n",
    "    plot4 = ax2.plot(range(0, len(val_loss)),\n",
    "                     val_loss,\n",
    "                     label='val_loss')\n",
    "\n",
    "    ax2.set(title='Loss And Val loss progress',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss/ validation loss')\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle('Result Of Model', fontsize=20, fontweight='bold')\n",
    "    fig.savefig('Accuracy_Loss_figure.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b1a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gráficas\n",
    "visualize_result = Visualize_Result(history.history['accuracy'], history.history['val_accuracy'], history.history['loss'],\n",
    "                                    history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "output = model.predict(testing_padded)\n",
    "output = np.array(np.round(output))\n",
    "matrix = confusion_matrix(test_labels,output)\n",
    "df = pd.DataFrame(matrix)\n",
    "tp, fp, fn, tn = matrix.ravel()\n",
    "# print(tn, ' ',tp, ' ',fn, ' ',fp)\n",
    "accuracy = (tp + tn)/(tp + tn + fn + fp)\n",
    "f1_score = (2*tp)/(2*tp + fp + fn)\n",
    "with open(\"acuracy_test.txt\",'a') as f:\n",
    "    f.write(str(accuracy)+\" \")\n",
    "with open(\"acuracy_val.txt\", 'a') as f:\n",
    "    f.write(str(list(history.history['val_accuracy'])[-1])+\" \")\n",
    "print(\"Test_acc: \",accuracy)\n",
    "print(\"Test_f1_score: \",f1_score)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fca8ee",
   "metadata": {},
   "source": [
    "# Asignación de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word_dic = {(0.55,1.1):'Bueno',(0.,0.55):'Malo'}\n",
    "output = np.around(model.predict(testing_padded),1)\n",
    "smto_wrds = []\n",
    "for task in output:\n",
    "    for key,value in word_dic.items():\n",
    "        #print('Este es el intervalo: '+str(key),'Esta es la palabra: '+str(value))\n",
    "        #print(str(task)+'JAJA'+str(key))\n",
    "        if task >= key[0] and task < key[1]:\n",
    "            #print(value)\n",
    "            smto_wrds.append(value)    \n",
    "            \n",
    "print(len(smto_wrds))\n",
    "# print(smto_wrds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa9a88",
   "metadata": {},
   "source": [
    "# Predicciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8862b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comprobación de los resultados\n",
    "output = model.predict(testing_padded)\n",
    "w = 0\n",
    "for test in test_data:\n",
    "    print('['+str(smto_wrds[w])+']',output[w],test,'\\n')\n",
    "    w += 1\n",
    "#     print(output[896-w],test_data[9312-w],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d79a9",
   "metadata": {},
   "source": [
    "# Funciones Varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "detokenizer = Detok()\n",
    "# print(str(stopwords.words('spanish')))\n",
    "stpwrds = detokenizer.detokenize(stopwords.words('spanish'))\n",
    "punct = wVord_tokenize(string.punctuation)\n",
    "print(string.punctuation)\n",
    "wordcloud = WordCloud(background_color='black', width=3400,\n",
    "                          height=2000).generate(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "archivo = 'BBDD_END_FIXED.xlsx'\n",
    "df = pd.read_excel(archivo, sheet_name='Coms_train')\n",
    "df = shuffle(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
